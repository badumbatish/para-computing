% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the lecture slides for
%%%% `Parallel Computing'
%%%% by Victor Eijkhout, copyright 2020
%%%%
%%%% Bigdata-slides.tex : slides about messages beyond 2G
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{numberedframe}{Overview}
  This section discusses big messages.

  Commands learned:
  \begin{itemize}
  \item \indexmpishow{MPI_Get_elements_x}
  \item \indexmpishow{MPI_Type_get_extent_x},
    \indexmpishow{MPI_Type_get_true_extent_x}
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{The problem with large messages}
  \begin{itemize}
  \item There is no problem allocating large buffers:
\begin{lstlisting}
size_t bigsize = 1<<33;
double *buffer = (double*) malloc(bigsize*sizeof(double));
\end{lstlisting}
\item But you can not tell MPI how big the buffer is:
\begin{lstlisting}
MPI_Send(buffer,bigsize,MPI_DOUBLE,...) // WRONG
\end{lstlisting}
because the size argument has to be \lstinline{int}.
  \end{itemize}
\end{numberedframe}

\begin{numberedframe}{Trick: use semi-large types}
  Make a derived datatype, and send a couple of those:
  
  \cverbatimsnippet{bigvectorptp}

  You can even receive them:

  \cverbatimsnippet{bigvectorrecv}
\end{numberedframe}

\begin{numberedframe}{So what's the problem?}
  \begin{itemize}
  \item Remember that in a receive call the buffer size is an upper bound.
  \item You use \lstinline{MPI_Get_count} to ask how much you got
  \item \ldots~that tells you how many semi-big objects there were
  \item There is also \lstinline{MPI_Get_elements} that counts the underlying type
  \item \ldots~and returns an \lstinline{int}.
  \end{itemize}
\end{numberedframe}

\protoslide{MPI_Get_elements}

\begin{numberedframe}{Large int counting}
  New to MPI-3: 
  \begin{itemize}
  \item Datatype \lstinline{MPI_Count}
  \item Routines such as \lstinline{MPI_Get_elements_x}
  \end{itemize}
  \cverbatimsnippet{bigvectorq}

  MPI-4 will use \lstinline{MPI_Count} parameters everywhere,\\
  in Fortran through polymorphism, in~C through \n{_c} suffix.
\end{numberedframe}

\protoslide{MPI_Get_elements_x}

\begin{numberedframe}{For your amusement}
What do you get if you print the following:
\cverbatimsnippet{compsizet}
?
\end{numberedframe}

\begin{numberedframe}{Big types}
  By composing types you can make a `big type'. Use
  \begin{itemize}
  \item \indexmpishow{MPI_Type_get_extent_x},
    \indexmpishow{MPI_Type_get_true_extent_x}
  \end{itemize}
  to query.
  These return \lstinline{MPI_Count} instead of \lstinline{int}.
\end{numberedframe}

\begin{numberedframe}{Big data in python}
  The \n{mpi4py} interface uses Python integers,\\
  which are 8 bytes. So the \n{_x} routines are not needed.
\end{numberedframe}

\endinput

\begin{numberedframe}{}
\begin{lstlisting}
\end{lstlisting}
\end{numberedframe}

