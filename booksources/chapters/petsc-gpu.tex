% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-2021
%%%%
%%%% petsc-objects.tex : petsc tangible-ish objects
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\Level 0 {Installation with GPUs}

PETSc can be configured with options
\begin{verbatim}
--with-cuda   --with-cudac=nvcc?
\end{verbatim}
You can test the presence of CUDA with:

\cverbatimsnippet{petsccudavar}

Some GPUs can accomodate MPI by being directly connected to the network
through \indexterm{GPUDirect} RDMA. If not:
\begin{verbatim}
-use_gpu_aware_mpi 0
\end{verbatim}

\Level 0 {Setup for GPU}

GPUs need to be initialized.
This can be done implicitly when a GPU object is created,
or explicitly through \indexpetscdef{PetscCUDAInitialize}.

\cverbatimsnippet{pcudainit}

\Level 0 {Distributed objects}

Dense matrices:
\indexpetscdef{MatCreateDenseCUDA},
\indexpetscdef{MatCreateSeqDenseCUDA},
giving types
\indexpetscdef{MATMPIDENSECUDA},
\indexpetscdef{MATDENSECUDA},
\indexpetscdef{MATAIJCUSPARSE}

Also
\indexpetscdef{VecCreateSeqCUDA},
\indexpetscdef{VecCreateMPICUDAWithArray},
\indexpetscdef{VECCUDA},
\indexpetscdef{VECSEQCUDA},
\indexpetscdef{VECMPICUDA}.

All sorts of `array' operations such as
\indexpetscdef{MatDenseCUDAGetArray},
\indexpetscdef{VecCUDAGetArray},

Set \indexpetscshow{PetscMalloc} to use the GPU:
\indexpetscdef{PetscMallocSetCUDAHost},
and switch back with
\indexpetscdef{PetscMallocResetCUDAHost}.

\Level 0 {Other}

Allocation: section~\ref{sec:petsc-malloc-gpu}.
