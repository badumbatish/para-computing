% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-2020
%%%%
%%%% mpi-moredata.tex : more about data
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\Level 0 {Type matching}

With derived types, you saw that it was not necessary for
the type of the sender and receiver to match.
Howver, when the send buffer is constructed,
and the receive buffer unpacked,
it is necessary for the successive types in that buffer to match.

The types in the send and receive buffers also need to match
the datatypes of the underlying architecture, with two exceptions.
The \indexmpishow{MPI_PACKED} and \indexmpishow{MPI_BYTE} types
can match any underlying type.
However, this still does not mean that these types can be used
on only sender or receiver, and a specific type on the other.

\Level 0 {Type size}
\label{sec:mpi-type-size}

\Level 1 {Matching MPI and language type sizes}
\label{sec:mpi-type-match}

The size of a datatype is not always statically known, for instance if
the Fortran \indextermtt{KIND} keyword is used. The translation of
datatypes in the source language can be translated to MPI types with
%
\indexmpiref{MPI_Type_match_size}
%
where the \lstinline{typeclass} argument is one of
\indexmpishow{MPI_TYPECLASS_REAL},
\indexmpishow{MPI_TYPECLASS_INTEGER},
\indexmpishow{MPI_TYPECLASS_COMPLEX}.

\cverbatimsnippet[examples/mpi/c/typematch.c]{typematchc}

In Fortran, the size of the datatype in the language can be obtained with
\indexmpishow{MPI_Sizeof} (note the non-optional error parameter!).
This routine is deprecated in \indexterm{MPI-4}: use of
\indextermtt{storage_size} and/or \indextermtt{c_sizeof} is recommended.

\fverbatimsnippet[examples/mpi/c/typematch.c]{typematchf}

The space that MPI takes for a structure type can be queried in a
variety of ways. First of all \indexmpiref{MPI_Type_size} counts the
\emph{datatype size}\index{MPI!datatype!size} as the 
number of bytes occupied by the data in a type. That means that in an
\emph{MPI vector datatype}\index{MPI!datatype!vector} it does not
count the gaps.
%
\cverbatimsnippet[examples/mpi/c/typesize.c]{vectortypesize}

\Level 1 {Extent and true extent}

The \emph{datatype extent}\index{MPI!datatype!extent},
measured with
\indexmpixref{MPI_Type_get_extent}{MPI_Type_get_true_extent},
is strictly the distance from the
first to the last data item of the type,
that is, with counting the gaps in the type.
It is measured in bytes so the output parameters are
of type \lstinline+MPI_Aint+.
%
\cverbatimsnippet[examples/mpi/c/typeextent.c]{vectortypeextent}

Similarly, using \indexmpishow{MPI_Type_get_extent} counts the gaps
in a \lstinline{struct} induced by \indexterm{alignment} issues.
%
\cverbatimsnippet[examples/mpi/c/struct.c]{structextent}
%
See section~\ref{sec:data:struct} for the code defining the structure type.

\begin{remark}
  Routine \indexmpishow{MPI_Type_get_extent}
  replaces
  deprecated functions \indexmpidef{MPI_Type_extent},
  \indexmpidef{MPI_Type_lb}, \indexmpidef{MPI_Type_ub}.
\end{remark}

The \emph{subarray datatype}\index{MPI!datatype!subarray} need not
start at the first element of the buffer, so the extent is an
overstatement of how much data is involved. The routine
\indexmpiref{MPI_Type_get_true_extent} returns the lower bound,
indicating where the data starts, and the extent from that point.

\begin{comment}
  Suppose we implement gather (see also Section Gather ) as a spanning
  tree implemented on top of point-to-point routines. Since the receive
  buffer is only valid on the root process, one will need to allocate
  some temporary space for receiving data on intermediate
  nodes. However, the datatype extent cannot be used as an estimate of
  the amount of space that needs to be allocated, if the user has
  modified the extent, for example by using MPI_TYPE_CREATE_RESIZED. The
  functions MPI_TYPE_GET_TRUE_EXTENT and MPI_TYPE_GET_TRUE_EXTENT_X are
  provided which return the true extent of the datatype.
\end{comment}

\cverbatimsnippet[examples/mpi/c/trueextent.c]{trueextent}

There is also a `big data' routine
\indexmpishow{MPI_Type_get_true_extent_x}.

\Level 1 {Extent resizing}

A type is partly characterized by its lower bound and extent,
or equivalently lower bound and upperbound.
Somewhat miraculously, you can actually change these to achieve special effects.

Consider for instance figure~\ref{fig:stridesend} and exercise~\ref{ex:stridesend}.
There, strided data was sent in individual transactions.
Would it be possible to address all these interleaved packets in one
gather or scatter call?

The problem here is that MPI uses the extent of the send type in a scatter,
or the receive type in a gather: if that type is 20 bytes big from
its first to its last element, then
data will be read out 20 bytes apart in a scatter, or written 20 bytes apart
in a gather. This ignores the `gaps' in the type!

The technicality on which the solution hinges is that you can `resize' a type
to give it a different extent, while not affecting how much data there
actually is in it.

Let's consider an example where each process makes a
buffer of integers that will be interleaved in a gather call:
\begin{lstlisting}
int *mydata = (int*) malloc( localsize*sizeof(int) );
for (int i=0; i<localsize; i++)
  mydata[i] = i*nprocs+procno;
MPI_Gather( mydata,localsize,MPI_INT,
    /* rest to be determined */ );
\end{lstlisting}

An ordinary gather call will of course not interleave, but
put the data end-to-end:
\begin{lstlisting}
MPI_Gather( mydata,localsize,MPI_INT,
            gathered,localsize,MPI_INT, // abutting
            root,comm );
\end{lstlisting}
\begin{verbatim}
gather 4 elements from 3 procs:
 0 3 6 9 1 4 7 10 2 5 8 11
\end{verbatim}

Using a strided type still puts data end-to-end, but now there
are unwritten gaps in the gather buffer:
\begin{lstlisting}
MPI_Gather( mydata,localsize,MPI_INT,
            gathered,1,stridetype, // abut with gaps
            root,comm );  
\end{lstlisting}
\begin{verbatim}
 0 1879048192 1100361260 3 3 0 6 0 0 9 1 198654
\end{verbatim}

The trick is to use \indexmpidef{MPI_Type_create_resized}
to make the extent of the type only one int long:
%
\cverbatimsnippet[examples/mpi/c/interleavegather]{interleavegather}
%
Now data is written with the same stride, but
at starting points equal to the shrunk extent:
\begin{verbatim}
 0 1 2 3 4 5 6 7 8 9 10 11
\end{verbatim}

\Level 2 {Example: dynamic vectors}

Does it bother you (a little) that in the vector type you
have to specify explicitly how many blocks there are?
It would be nice if you could create a `block with padding'
and then send however many of those.

Well, you can introduce that padding by resizing a type,
making it a little larger.

\cverbatimsnippet[examples/mpi/c/stridestretch.c]{paddedblock1}

There is a second solution to this problem, using a structure type.
This does not use resizing, but rather indicates a displacement
that reaches to the end of the structure. We do this
by putting a type \indexmpishow{MPI_UB} at this displacement:

\cverbatimsnippet[examples/mpi/c/stridestretch.c]{paddedblock2}

\Level 2 {Example: transpose}
\index{transpose!through derived types|(}

\begin{figure}[ht]
  \includegraphics[scale=.3]{transposetype}
  \caption{Transposing a 1D partitioned array}
  \label{fig:transposetype} 
\end{figure}
Transposing data is an important part of such operations as the \ac{FFT}.
We develop this in steps. Refer to figure~\ref{fig:transposetype}.

The source data is easily described as a vector type defined as:
\begin{itemize}
\item there are $b$ blocks,
\item of blocksize $b$,
\item spaced apart by the global $i$-size of the array.
\end{itemize}
% this is an exercise, so no source listed
\cverbatimsnippet{transposesrctype} % in an exerccise

The target type is harder to describe.
First we note that each contiguous block from the source type
can be described as a vector type with:
\begin{itemize}
\item $b$ blocks,
\item of size~1 each,
\item stided by the global $j$-size of the matrix.
\end{itemize}
%
\cverbatimsnippet{transposetarlinetype}% in an exercise

For the full type at the receiving process we now need to pack
$b$ of these lines together.

\begin{exercise}
  Finish the code.
  \begin{itemize}
  \item
    What is the extent of the \lstinline{targetcolumn} type?
  \item What is the spacing of the first elements of the blocks? How
    do you therefore resize the \lstinline{targetcolumn} type?
  \end{itemize}
\end{exercise}

\index{transpose!through derived types|)}

\Level 0 {More about data}

\begin{comment}
  \Level 1 {Datatype signatures}
  \label{sec:signature}
  \index{datatype!signature|(}

  With the primitive types it pretty much went
  without saying that if the sender sends an array of doubles, the
  receiver had to declare the datatype also as doubles. With derived
  types that is no longer the case: the sender and receiver can declare
  a different datatype for the send and receive buffer, as long as these
  have the same \indextermbus{datatype}{signature}.

  The signature of a datatype is the internal representation of that
  datatype. For instance, if the sender declares a datatype consisting
  of two doubles, and it sends four elements of that type, the receiver
  can receive it as two elements of a type consisting of four doubles.

  You can also look at the signature as the form `under the hood' in which MPI
  sends the data.

  \index{datatype!signature|)}
\end{comment}

\Level 1 {Big data types}
\index{datatype!big|(}

The \n{size} parameter in MPI send and receive calls is of type integer,
meaning that it's maximally~$2^{31}-1$. These day computers are big enough
that this is a limitation. Derived types offer some way out: to send
a \emph{big data type} of $10^{40}$ elements you would
\begin{itemize}
\item create a contiguous type with $10^{20}$ elements, and
\item send $10^{20}$ elements of that type.
\end{itemize}
This often works, but it's not perfect. For instance, the routine
\indexmpishow{MPI_Get_elements} returns the total number of basic elements sent
(as opposed to \indexmpishow{MPI_Get_count} which would return the number
of elements of the derived type). Since its output argument is
of integer type, it can't store the right value.

The \mpistandard{3} standard has addressed this
through the introduction of an \indexmpishow{MPI_Count} datatype,
and new routines that return that type of count.
(The alternative would be to break backwards compatibility
and use \indexmpishow{MPI_Count} parameter in all existing routines.)

Let us consider an example.

Allocating a buffer of more than 4Gbyte is not hard:
\cverbatimsnippet[examples/mpi/c/vectorx.c]{bigvectoralloc}

We use the trick with sending elements of a derived type:
\cverbatimsnippet[examples/mpi/c/vectorx.c]{bigvectorptp}

We use the same trick for the receive call, but now we catch the status
parameter which will later tell us how many elements of the basic type
were sent:
%
\cverbatimsnippet[examples/mpi/c/vectorx.c]{bigvectorrecv}

When we query how many of the basic elements are in the buffer
(remember that in the receive call the buffer length is
an upper bound on the number of elements received)
do we
need a counter that is larger than an integer.  MPI has introduced a
type \indexmpidef{MPI_Count} for this, and new routines such as
\indexmpixref{MPI_Get_elements_x}{MPI_Get_elements} that return a
count of this type:

\cverbatimsnippet[examples/mpi/c/vectorx.c]{bigvectorq}

\begin{remark}
  Computing a big number to allocate is not entirely simple.
  \cverbatimsnippet[examples/mpi/c/getx.c]{compsizet}
  gives as output:
\begin{verbatim}
size of size_t = 8
0 68719476736 68719476736 0 68719476736
\end{verbatim}
Clearly, not only do operations go left-to-right, but casting is done that way too:
the computed subexpressions are only cast to \lstinline{size_t} if one operand is.
\end{remark}

Above, we did not actually create a datatype that was bigger than~2G,
but if you do so, you can query its extent by
\indexmpixref{MPI_Type_get_extent_x}{MPI_Type_get_extent}
and
\indexmpixref{MPI_Type_get_true_extent_x}{MPI_Type_get_true_extent}.

\begin{pythonnote}{Big data}
  Since python has unlimited size integers there is
  no explicit need for the `x' variants of routines.
  Internally, \lstinline+MPI.Status.Get_count+ is implemented
  in terms of \indexmpishow{MPI_Get_count_x}.
\end{pythonnote}

\index{datatype!big|)}

\Level 1 {Packing}
\label{sec:pack}

One of the reasons for derived datatypes is dealing with non-contiguous data.
In older communication libraries this could only be done by \indexterm{packing} data
from its original containers into a buffer, and likewise unpacking it at the
receiver into its destination data structures.

MPI offers this packing facility, partly for compatibility with such libraries,
but also for reasons of flexibility. Unlike with derived datatypes,
which transfers data atomically, packing routines add data sequentially
to the buffer and unpacking takes them sequentially. 

This means that 
one could pack an integer describing how many floating point numbers
are in the rest of the packed message. 
Correspondingly, the unpack routine could then investigate the first integer
and based on it unpack the right number of floating point numbers.

MPI offers the following:
\begin{itemize}
\item The \indexmpishow{MPI_Pack} command adds data to a send buffer;
\item the \indexmpishow{MPI_Unpack} command retrieves data from a receive buffer;
\item the buffer is sent with a datatype of \indexmpishow{MPI_PACKED}.
\end{itemize}

With \indexmpishow{MPI_Pack} data elements can be added 
to a buffer one at a time. The \n{position} parameter is updated
each time by the packing routine.
\begin{lstlisting}
int MPI_Pack(
  void *inbuf, int incount, MPI_Datatype datatype,
  void *outbuf, int outcount, int *position,
  MPI_Comm comm);
\end{lstlisting}

Conversely, \indexmpishow{MPI_Unpack} retrieves one element
from the buffer at a time. You need to specify the MPI datatype.
\begin{lstlisting}
int MPI_Unpack(
  void *inbuf, int insize, int *position,
  void *outbuf, int outcount, MPI_Datatype datatype,
  MPI_Comm comm);
\end{lstlisting}

A packed buffer is sent or received with a datatype of
\indexmpishow{MPI_PACKED}. The sending routine uses the \n{position}
parameter to specify how much data is sent, but the receiving routine
does not know this value a~priori, so has to specify an upper bound.

\cverbatimsnippet[examples/mpi/pack/pack.c]{packunpack}

You can precompute the size of the required buffer with
%
\indexmpiref{MPI_Pack_size}
%
Add one time \indexmpishow{MPI_BSEND_OVERHEAD}.

\begin{exercise}
  \label{ex:packAOS}
  Suppose you have a `structure of arrays'
\begin{lstlisting}
struct aos {
  int length;
  double *reals;
  double *imags;
};
\end{lstlisting}
  with dynamically created arrays. Write code to send and receive this
  structure.
\end{exercise}
