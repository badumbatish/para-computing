% -*- latex -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%
%%%% This text file is part of the source of 
%%%% `Parallel Programming in MPI and OpenMP'
%%%% by Victor Eijkhout, copyright 2012-2020
%%%%
%%%% mpi-persist.tex : persistent communication and such
%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\Level 0 {Persistent communication requests}
\index{persistent communication|see{communication, persistent}}
\label{sec:persistent}

Persistent communication is a mechanism for dealing
with a repeating communication transaction,
where are parameters of the transaction
stay the same, such as sender, receiver, tag, root, and buffer type and size.
Only the contents of the buffers involved changes between the transactions.

You can imagine that setting up a communication
carries some overhead, and if the same communication structure
is repeated many times, this overhead may be avoided by reusing
the request object.

\begin{enumerate}
\item
  For both point-to-point and collective (non-blocking) communications
  \n{MPI_Ixxx}
  there is a persistent variant \n{MPI_Xxx_init}
  with the same calling sequence.
  In particular, the `init' call produces
  an \indexmpishow{MPI_Request} output parameter.
\item 
  The `init' routine does not start the actual communication:
  that is done in
  \indexmpidef{MPI_Start},
  or \indexmpidef{MPI_Startall} for multiple requests.
\item Any of the \n{MPI_Waitxxx} calls can then be used
  to conclude the communication.
\item The communication can then be restarted with another `start' call.
\item The wait call does not release the request object: that is done
  with \indexmpishow{MPI_Request_free}.
\end{enumerate}

\begin{lstlisting}
MPI_Send_init( /* ... */ &request);
while ( /* ... */ ) {
  MPI_Start( request );
  MPI_Wait( request, &status );
}
MPI_Request_free( & request );
\end{lstlisting}

\begin{mplnote}{Persistent requests}
  MPL returns a \indexmpldef{prequest}
  from persistent `init' routines, rather than an \indexmplshow{irequest}
  (MPL note~\ref{mpl:irequest}):
\begin{lstlisting}
template<typename T >
prequest send_init (const T &data, int dest, tag t=tag(0)) const;
\end{lstlisting}
Likewise, there is a \indexmpldef{prequest_pool}
instead of an \indexmplshow{irequest_pool} (note~\ref{mpl:req_pool}).
\end{mplnote}

\Level 1 {Persistent point-to-point communication}
\index{persistent!point-to-point|(textbf}

The main persistent point-to-point routines are
\indexmpishow{MPI_Send_init}, which has the same calling sequence as
  \indexmpishow{MPI_Isend}, and \indexmpishow{MPI_Recv_init}, which has the same
  calling sequence as \indexmpishow{MPI_Irecv}.
  
After a request object has been used, possibly multiple times, it can
be freed; see~\ref{ref:mpirequest}.

In the following example a ping-pong is implemented with persistent communication.
\cverbatimsnippet[examples/mpi/c/persist.c]{persist}
%
\pverbatimsnippet[examples/mpi/p/persist.py]{persistp}

As with ordinary send commands, there are persistent variants
\indexmpishow{MPI_Bsend_init},
\indexmpishow{MPI_Ssend_init},
\indexmpishow{MPI_Rsend_init}.

\index{persistent!point-to-point|)}

\Level 1 {Persistent collectives}

\begin{mpifour}
% https://github.com/mpi-forum/mpi-issues/issues/25
\index{persistent!collectives|textbf(}
For each collective call, there is a persistent variant (\mpistandard{4}).
This variant has the same calling sequence, except for a final
\indexmpishow{MPI_Request} parameter.
This request (or an array of requests) can then be used by
\indexmpishow{MPI_Start} (or \indexmpishow{MPI_Startall})
to initiate the actual communication.

Some points.
\begin{itemize}
\item
  Metadata arrays, such as of counts and datatypes,
  must not be altered until the \indexmpishow{MPI_Request_free} call.
\item The initialization call is non-local, so it can block until all
  processes have performed it.
\item Multiple persistent collective can be initialized, in which case
  they satisfy the same restrictions as ordinary collectives, in particular
  on ordering. Thus, the following code is incorrect:
\begin{lstlisting}
// WRONG
if (procid==0) {
  MPI_Reduce_init( /* ... */ &req1);
  MPI_Bcast_init( /* ... */ &req2);
} else {
  MPI_Bcast_init( /* ... */ &req2);
  MPI_Reduce_init( /* ... */ &req1);
}
\end{lstlisting}
However, after initialization the start calls can be in arbitrary order,
and in different order among the processes.
\end{itemize}

\begin{raggedright}
  Available persistent collectives are:
  \indexmpidef{MPI_Barrier_init}
  \indexmpidef{MPI_Bcast_init}
  \indexmpidef{MPI_Reduce_init}
  \indexmpidef{MPI_Allreduce_init}
  \indexmpidef{MPI_Reduce_scatter_init}
  \indexmpidef{MPI_Reduce_scatter_block_init}
  \indexmpidef{MPI_Gather_init}
  \indexmpidef{MPI_Gatherv_init}
  \indexmpidef{MPI_Allgather_init}
  \indexmpidef{MPI_Allgatherv_init}
  \indexmpidef{MPI_Scatter_init}
  \indexmpidef{MPI_Scatterv_init}
  \indexmpidef{MPI_Alltoall_init}
  \indexmpidef{MPI_Alltoallv_init}
  \indexmpidef{MPI_Alltoallw_init}
  \indexmpidef{MPI_Scan_init}
  \indexmpidef{MPI_Exscan_init}
\end{raggedright}

\index{persistent!collectives|)}
\end{mpifour}

\Level 1 {Persistent neighbor communications}

\begin{mpifour}

\indexmpidef{MPI_Neighbor_allgather_init},
\indexmpidef{MPI_Neighbor_allgatherv_init},
\indexmpidef{MPI_Neighbor_alltoall_init},
\indexmpidef{MPI_Neighbor_alltoallv_init},
\indexmpidef{MPI_Neighbor_alltoallw_init},

\end{mpifour}

\Level 0 {Buffered communication}
\label{sec:buffered}

\begin{figure}[ht]
  \includegraphics[scale=.5]{bufferattach}
  \caption{User communication routed through an attached buffer}
  \label{fig:bufattach}
\end{figure}

By now you have probably got the notion that managing buffer
space in MPI is important: data has to be somewhere, either in
user-allocated arrays or in system buffers. Using
\indextermsubdef{buffered}{communication} is yet another
way of managing buffer space.
\begin{enumerate}
\item You allocate your own buffer space, and you attach it to your
  process. This buffer is not a send buffer: it is a replacement for
  buffer space used inside the MPI library or on the network card;
  figure~\ref{fig:bufattach}. If high-bandwdith memory is available,
  you could create your buffer there.
\item You use the \indexmpiref{MPI_Bsend}
  (or its \emph{local}\index{local!operation} variant \indexmpishow{MPI_Ibsend})
  call for sending, using
  otherwise normal send and receive buffers;
\item You detach the buffer when you're done with the buffered sends.
\end{enumerate}

One advantage of buffered sends is that they are non-blocking:
since there is a guaranteed buffer long enough to contain the
message, it is not necessary to wait for the receiving process.

We illustrate the use of buffered sends:

\cverbatimsnippet{bsendbuf}

\Level 1 {Buffer treatment}

There can be only one buffer per process, attached with
\indexmpiref{MPI_Buffer_attach}.
Its size should be enough
for all \indexmpishow{MPI_Bsend} calls that are simultaneously
outstanding.
You can compute the needed size of the buffer with \indexmpishow{MPI_Pack_size};
see section~\ref{sec:pack}.
Additionally, a term of \indexmpidef{MPI_BSEND_OVERHEAD} is needed.
See the above code fragment.

The buffer is detached with \indexmpishow{MPI_Buffer_detach}:
\begin{lstlisting}
int MPI_Buffer_detach(
  void *buffer, int *size);
\end{lstlisting}
This returns the address and size of the buffer; the call blocks
until all buffered messages have been delivered.

Note that both
\indexmpishow{MPI_Buffer_attach} and \indexmpishow{MPI_Buffer_detach}
have a \lstinline+void*+ argument for the buffer, but 
\begin{itemize}
\item in the attach routine this is the address of the buffer,
\item while the detach routine it is the address of the buffer pointer.
\end{itemize}
This is done so that the detach routine can zero the buffer pointer.

While the buffered send is non-blocking like an \indexmpishow{MPI_Isend},
there is no corresponding wait call.
You can force delivery by
\begin{lstlisting}
MPI_Buffer_detach( &b, &n );
MPI_Buffer_attach( b, n );
\end{lstlisting}

\begin{mplnote}{Buffered send}
  Creating and attaching a buffer is done through \indexmpldef{bsend_buffer}
  and a support routine \indexmpldef{bsend_size} helps in calculating
  the buffer size:
  \cxxverbatimsnippet{bsendbufmpl}
\end{mplnote}

\begin{mplnote}{Buffer attach and detach}
  There is a separate attach routine, but normally this is called
  by the constructor of the \lstinline+bsend_buffer+.
  Likewise, the detach routine is called in the buffer destructor.
\begin{lstlisting}
void mpl::environment::buffer_attach (void *buff, int size);
std::pair< void *, int > mpl::environment::buffer_detach ();
\end{lstlisting}
\end{mplnote}

\Level 1 {Bufferend send calls}

The possible error codes are
\begin{itemize}
\item \indexmpishow{MPI_SUCCESS} the routine completed successfully.
\item \indexmpishow{MPI_ERR_BUFFER} The buffer pointer is invalid;
  this typically means that you have supplied a null pointer.
\item \indexmpishow{MPI_ERR_INTERN} An internal error in MPI has been detected.
\end{itemize}

The asynchronous version is \indexmpishow{MPI_Ibsend}, the persistent
(see section~\ref{sec:persistent}) call is \indexmpishow{MPI_Bsend_init}.

\Level 1 {Persistent buffered communication}

There is a persistent variant 
\indexmpiref{MPI_Bsend_init}
of buffered sends, as with regular
sends (section~\ref{sec:persistent}).

